{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impostazione iniziale codice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# variables reset\n",
    "%reset -f\n",
    "\n",
    "# various imports\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "from keras import layers, models\n",
    "import keras \n",
    "import os\n",
    "import tensorflow\n",
    "import numpy\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "numpy.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# obtaining BERT models\n",
    "models = [\"cardiffnlp/twitter-roberta-large-emotion-latest\",\n",
    "          \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\",\n",
    "          \"sileod/deberta-v3-small-tasksource-nli\",\n",
    "          \"dslim/distilbert-NER\"\n",
    "          #\"answerdotai/ModernBERT-large\",\n",
    "          ]\n",
    "\n",
    "\n",
    "md = []\n",
    "\n",
    "# obtaining tokenizer and pre-trained BERT models from huggingface\n",
    "for model in models:\n",
    "\n",
    "    # this code is to make the loading entirely offline once downloaded(edit USER string)\n",
    "    #model = (\"models/\"+model).replace(\"/\",\"--\")\n",
    "    #cached = \"C:\\\\Users\\\\USER\\\\.cache\\\\huggingface\\\\hub\\\\\"\n",
    "    #model = cached+model+\"\\\\snapshots\\\\\"\n",
    "    #model = model +\"\\\\\"+os.listdir(model)[0] + \"\\\\\"\n",
    "\n",
    "    # download of the models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)#,local_files_only=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model,output_hidden_states=True)#,local_files_only=True)\n",
    "\n",
    "    #appending them for later use\n",
    "    md.append((tokenizer,model))\n",
    "\n",
    "base_path = os.path.dirname(os.path.abspath(\"__file__\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting dataset base paths\n",
    "For internal usage\n",
    "* test -> dataset for doing various tests(like thresholding, etc, acts like a validation)\n",
    "* dev -> dataset on witch final predictions are done(the one sent to evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the training dataset\n",
    "trainpath = base_path + \"\\\\dataset\"\n",
    "train_docs = trainpath + \"\\\\raw-documents\"\n",
    "traintxt = trainpath+\"\\\\train.txt\"\n",
    "trainjson = \"train.json\"\n",
    "\n",
    "# this is the final test dataset for the predictions\n",
    "devpath = base_path + \"\\\\dataset\"\n",
    "dev_docs = devpath + \"\\\\raw-documents\"\n",
    "devtxt = devpath+\"\\\\dev.txt\"\n",
    "devjson = \"dev.json\"\n",
    "\n",
    "# this is a dataset on witch some tests are done \n",
    "testpath = base_path + \"\\\\dataset\"\n",
    "test_docs = testpath + \"\\\\raw-documents\"\n",
    "testtxt = testpath+\"\\\\test.txt\"\n",
    "testjson = \"test.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "Here, we separate each sentence of a file into a single sample, we also process narrartives, subnarratives and argoument files for later use, beware that there could be some errors if the dataset isn't correctly formatted. to mimick 2024 dataset code, everything is converted to a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "pattern = r\"(.+)\\t(.+)\\t(.+)\"\n",
    "classes_pattern = r\"((URW)|(CC)):((\\w|\\s|[:,-\\/]|)+)\"\n",
    "win_max_len = 3\n",
    "\n",
    "# mode 0 -> dataset with classes\n",
    "# mode 1 -> dataset without classes(the one to send)\n",
    "\n",
    "def extractionTool(path,outfile,mode=0):\n",
    "    baselist = []\n",
    "    id = 0\n",
    "    with open(path,\"r\") as f:\n",
    "        # taking each line of the labeled file\n",
    "        for line in f:\n",
    "            v = \"\"\n",
    "            trigger = False\n",
    "            file = line.replace(\"\\n\",\"\")\n",
    "            classes = \"\"\n",
    "            subclasses = \"\"\n",
    "            superclass = \"\"\n",
    "            secondsuperclass = \"\"\n",
    "            if mode == 0:\n",
    "                # Narrative and subnarrative extraction\n",
    "                v = list(re.match(pattern,line).groups())\n",
    "                file = v[0]\n",
    "                classes = [match[3] for match in re.findall(classes_pattern, v[1])]\n",
    "                subclasses = [match[3] for match in re.findall(classes_pattern, v[2])]\n",
    "                superclass = [match[0] for match in re.findall(classes_pattern, v[1])]\n",
    "                # obtaining argument\n",
    "                if(len(superclass)==0):\n",
    "                    superclass = \"Other\"\n",
    "                    trigger = True\n",
    "                    secondsuperclass = \"URW\" if \"URW\" in line else \"CC\"\n",
    "                else:\n",
    "                    superclass = superclass[0]\n",
    "                if len(classes)!=len(subclasses):\n",
    "                    raise Exception()\n",
    "                if len(classes) == 0:\n",
    "                    classes.append(\" Other\")\n",
    "                    subclasses.append(\" Other\")\n",
    "                \n",
    "            print(f\"file:{file},{classes},{subclasses},{superclass}\")\n",
    "            # sentence split of all the sentences of a file\n",
    "            with open(train_docs+\"\\\\\"+file,\"r\",encoding=\"raw_unicode_escape\") as sub_file:\n",
    "                    val = \"\"\n",
    "                    window = []\n",
    "                    # extraction line by line\n",
    "                    for line in sub_file:\n",
    "                        if line != \"\\n\":\n",
    "                            # dictionary mapping for properties, such the main text, and a context window\n",
    "                            dic = {}\n",
    "                            dic[\"id\"] = id\n",
    "                            dic[\"text\"] = line\n",
    "                            dic[\"file\"] = file\n",
    "                            dic[\"classes\"] = [superclass +\":\" +x+\";\" for x in classes]\n",
    "                            dic[\"precedent_txt\"] = val\n",
    "                            dic[\"window_txt\"] = \"\".join(window)\n",
    "                            window.append(line)\n",
    "                            if len(window) > win_max_len:\n",
    "                                window.pop(0)\n",
    "                            val = val + line \n",
    "                            # sottoclassi\n",
    "                            dic[\"labels\"] = [superclass +\":\" +x+\";\" for x in subclasses]\n",
    "                            dic[\"superclass\"] = [superclass]\n",
    "                            if trigger:\n",
    "                                dic[\"superclass\"] = [superclass,secondsuperclass]\n",
    "                            dic[\"fulltext\"] = \"0\"\n",
    "                            id += 1\n",
    "                            baselist.append(dic)\n",
    "\n",
    "                    # separator containing the full text\n",
    "                    dic = {}\n",
    "                    dic[\"id\"] = id\n",
    "                    dic[\"text\"] = val\n",
    "                    dic[\"file\"] = file\n",
    "                    dic[\"classes\"] = [superclass +\":\" +x+\";\" for x in classes]\n",
    "                    dic[\"precedent_txt\"] = \"\"\n",
    "                    dic[\"window_txt\"] = \"\"\n",
    "                    \n",
    "                    # sottoclassi\n",
    "                    dic[\"precedent_txt\"] = val\n",
    "                    dic[\"labels\"] = [superclass +\":\" +x+\";\" for x in subclasses]\n",
    "                    dic[\"superclass\"] = [superclass]\n",
    "                    if trigger:\n",
    "                        dic[\"superclass\"] = [superclass,secondsuperclass]\n",
    "                    dic[\"fulltext\"] = \"1\"\n",
    "                    id += 1\n",
    "                    baselist.append(dic)\n",
    "            \n",
    "    with open(outfile+\".json\",\"w\") as f:\n",
    "        json.dump(baselist, f)\n",
    "\n",
    "extractionTool(traintxt,\"train\")\n",
    "extractionTool(testtxt,\"test\")\n",
    "extractionTool(devtxt,\"dev\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction of Generic Information from the dataset(single sentencies, not files), Including:\n",
    "\n",
    "* Number of classes\n",
    "* Various probabilities (some discarded)\n",
    "* Class labels\n",
    "* Statistics on the number of labels per instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# getting the classes\n",
    "with open(trainjson,\"r\",encoding=\"utf8\") as f:\n",
    "    trainreadjson = json.load(f)\n",
    "\n",
    "# putting all subnarratives and narratives in each argument\n",
    "superclassmap = {\"URW\":set(),\"CC\":set(),\"Other\":set()}\n",
    "var = set()\n",
    "meancalc = []\n",
    "for elem in trainreadjson:\n",
    "    for j in elem[\"labels\"]:\n",
    "        var.add(j)\n",
    "        superclassmap[elem[\"superclass\"][0]].add(j)\n",
    "\n",
    "print(\"aguments map:\")\n",
    "print(superclassmap)\n",
    "print()\n",
    "# from set to list, the set wss used to have only one copy of subnarratives\n",
    "var = sorted(var)\n",
    "prob = dict.fromkeys(var, 0)\n",
    "prob_2 = dict.fromkeys(range(20),0)\n",
    "total = 0\n",
    "for elem in trainreadjson:\n",
    "    prob_2[len(elem[\"labels\"])] += 1\n",
    "    for j in elem[\"labels\"]:\n",
    "        prob[j] += 1\n",
    "        total += 1\n",
    "for key in prob.keys():\n",
    "    prob[key] = prob[key]/total\n",
    "final_prob_vec = []\n",
    "\n",
    "print(\"probability of subnarratives in the dataset\")\n",
    "print(prob)\n",
    "print()\n",
    "\n",
    "superclassprob = {\"URW\":0,\"CC\":0,\"Other\":0}\n",
    "for elem in superclassprob.keys():\n",
    "    for pr in prob.keys():\n",
    "        if pr in superclassmap[elem]:\n",
    "            superclassprob[elem] += prob[pr]\n",
    "\n",
    "print(\"Argoument probabilities:\")\n",
    "print(superclassprob)\n",
    "print()\n",
    "\n",
    "for elem in var:\n",
    "    final_prob_vec.append(prob[elem])\n",
    "\n",
    "print(\"Number of classes for element:\\n\")\n",
    "print(prob_2)\n",
    "\n",
    "for elem in prob_2.keys():\n",
    "    prob_2[elem] = prob_2[elem]/total\n",
    "print(\"Number of classes for element normalized:\\n\")\n",
    "print(prob_2)\n",
    "#var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and one hot encoding\n",
    "This part extracts the dataset embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_embeddings(json):\n",
    "    leghjson = len(json)\n",
    "    extracted_unit = 0\n",
    "    # unused\n",
    "    context_previous_full_multiple_embedding = []\n",
    "    context_window_multiple_embedding = []\n",
    "    max_logical_window = 2\n",
    "\n",
    "    for value in json:\n",
    "\n",
    "        # various containers for an element of the dataset, some got unused\n",
    "        value[\"tokenized\"] = []\n",
    "        value[\"context_CLS_embedding\"] = []\n",
    "        value[\"context_window_multiple_embedding\"] = []\n",
    "        value[\"context_previous_full_multiple_embedding\"] = []\n",
    "        value[\"context_window_single_embedding\"] = []\n",
    "        value[\"context_previous_full_single_embedding\"] = []\n",
    "        value[\"context_CLS_as_much_models_can_take\"] = []\n",
    "        tokenized = None\n",
    "\n",
    "        # CLS embedding extraction\n",
    "        for mod in md:\n",
    "\n",
    "            # taking out tokenizer and model\n",
    "            tokenizer = mod[0]\n",
    "            model = mod[1]\n",
    "\n",
    "            # extraction of main sentence embedding\n",
    "            try:\n",
    "                tokenized = tokenizer(value[\"text\"],return_tensors=\"pt\",max_length=512)\n",
    "                v = model(**tokenized)\n",
    "            except:\n",
    "                print(\"cut\")\n",
    "                # handling of too long texts, i cut them in some parts and take out the middle/last part\n",
    "                leng_token = len(value[\"text\"])//2\n",
    "                tokenized = tokenizer(value[\"text\"][leng_token:2*leng_token],return_tensors=\"pt\",max_length=512)\n",
    "                v = model(**tokenized)\n",
    "            \n",
    "            # embedding extraction of the CLS\n",
    "            last_layer = v.hidden_states[-1][0].detach().numpy()\n",
    "\n",
    "            # appending to the list of embeddings\n",
    "            value[\"context_CLS_embedding\"].append(last_layer[0])\n",
    "            \n",
    "            # 2 precedent sentencies embedding, its similar to the precedent code\n",
    "            try:\n",
    "               tokenized = tokenizer(value[\"window_txt\"],return_tensors=\"pt\",truncation=True)\n",
    "               v = model(**tokenized)\n",
    "            except:\n",
    "               print(\"cut\")\n",
    "               # handling di testi troppo lunghi, estraggo la parte centrale dato che si suppone la più importante\n",
    "               leng_token = len(value[\"window_txt\"])//2\n",
    "               tokenized = tokenizer(value[\"window_txt\"][leng_token:2*leng_token],return_tensors=\"pt\",truncation=True)\n",
    "               v = model(**tokenized)\n",
    "            last_layer = v.hidden_states[-1][0].detach().numpy()\n",
    "            value[\"context_window_single_embedding\"].append(last_layer[0])\n",
    "\n",
    "        # one hot encoding of argument, narrative and subnarrative\n",
    "        try:\n",
    "            # check if classes are present (its needed on submission dataset)\n",
    "            if value[\"labels\"]!=None and value[\"labels\"]!=\"\":\n",
    "\n",
    "                # argouent embedding, this is unused in final model\n",
    "                value[\"superclass_embedding\"] = [1 if x in value[\"superclass\"] else 0 for x in superclassmap.keys()]\n",
    "                if value[\"superclass_embedding\"][2] == 1:\n",
    "\n",
    "                    # other class splits the embedding values\n",
    "                    if value[\"superclass_embedding\"][1] == 1:\n",
    "                        value[\"superclass_embedding\"][1] == 0.5\n",
    "                    else:\n",
    "                        value[\"superclass_embedding\"][0] == 0.5\n",
    "\n",
    "                # this is in_argoument one hot encoding (so we are excluding the other argoumtn from this one hot encoding)\n",
    "                # unused\n",
    "                value[\"y_single_class\"] = [1 if x in value[\"labels\"] else 0 for x in superclassmap[value[\"superclass\"][0]]]\n",
    "                \n",
    "                # this is a global one hot encoding\n",
    "                # in final model this was used\n",
    "                value[\"y\"] = [1 if(x in value[\"labels\"]) else 0 for x in var]\n",
    "\n",
    "                # number of classes embedding, this is unused\n",
    "                sum = numpy.sum(value[\"y\"])\n",
    "                value[\"y_total\"] = [1 if(x+1 == sum) else 0 for x in range(len(prob_2.keys()))]\n",
    "\n",
    "            #print(value[\"y\"],value[\"labels\"])\n",
    "            #print(value[\"y_total\"],sum)\n",
    "        except:\n",
    "            pass\n",
    "        extracted_unit += 1\n",
    "        #percentuale di completamento estrazione\n",
    "        print(extracted_unit/leghjson)\n",
    "    \n",
    "\n",
    "# concatenation module\n",
    "\n",
    "# this function concatenates the embeddings for a single sample, this will give a dataset ready for keras\n",
    "# (single sentences, or single sentences + 2 precedent sentencies etc..) \n",
    "def convert_to_model(dataset,mode=0):\n",
    "    val2 = None\n",
    "    # single sentencies\n",
    "    if mode == 0:\n",
    "        val = \"context_CLS_embedding\"\n",
    "\n",
    "    # unused\n",
    "    elif mode == 1:\n",
    "        val = \"context_CLS_as_much_models_can_take\" \n",
    "\n",
    "    # main sentence + 2 precedding in the file\n",
    "    elif mode == 2:\n",
    "        val2 = \"context_window_single_embedding\" # window di 2\n",
    "        val = \"context_CLS_embedding\"\n",
    "    \n",
    "    # unused\n",
    "    elif mode == 3:\n",
    "        val = \"context_CLS_embedding\"\n",
    "        val2 = \"context_previous_full_single_embedding\"\n",
    "    \n",
    "    few = dataset\n",
    "    # main concatenation and setting the dataset for numpy and tensorflow usage\n",
    "    input_x = []\n",
    "    y = []\n",
    "    y_total = []\n",
    "\n",
    "    # for each sample \n",
    "    for v in few:\n",
    "        var = []\n",
    "        # list of one hot encoding for later use in keras\n",
    "        try:\n",
    "            # subnaratives \n",
    "            y.append(v[\"y\"])\n",
    "\n",
    "            # number of classes encoding \n",
    "            # unused\n",
    "            y_total.append(v[\"y_total\"])\n",
    "        except:\n",
    "            pass\n",
    "        # concatenation of all embeddings of a sample from all hugginface models\n",
    "        for j in v[val]:\n",
    "            var.append(j)\n",
    "        # concatenation of window in case of mode with window\n",
    "\n",
    "        if val2 != None:\n",
    "            for j in v[val2]:\n",
    "                var.append(j)\n",
    "\n",
    "        # conversion of list into a single numpy array (not matrix)\n",
    "        var = numpy.concatenate(var)\n",
    "        var = var.flatten()\n",
    "\n",
    "        # appending the array to the dataset\n",
    "        input_x.append(var)\n",
    "\n",
    "    # full dataset in keras\n",
    "    input_x = numpy.array(input_x)\n",
    "    y = numpy.array(y)\n",
    "\n",
    "    # unused\n",
    "    y_total = numpy.array(y_total)\n",
    "    return(input_x,y,y_total)\n",
    "\n",
    "# top-down appoach version\n",
    "# this its present in later code, but for the final model it was unused\n",
    "def convert_to_model_special(dataset,mode=0):\n",
    "    val2 = None\n",
    "    if mode == 0:\n",
    "        val = \"context_CLS_embedding\"\n",
    "    elif mode == 1:\n",
    "        val = \"context_CLS_as_much_models_can_take\" \n",
    "    elif mode == 2:\n",
    "        val2 = \"context_window_single_embedding\" \n",
    "        val = \"context_CLS_embedding\"\n",
    "    elif mode == 3:\n",
    "        val = \"context_CLS_embedding\"\n",
    "        val2 = \"context_previous_full_single_embedding\"\n",
    "\n",
    "    few = dataset\n",
    "    input_x = {\"URW\":list(),\"CC\":list(),\"Other\":list(),\"full\":list()}\n",
    "    y = {\"URW\":list(),\"CC\":list(),\"Other\":list()}\n",
    "    y_total = {\"URW\":list(),\"CC\":list(),\"Other\":list()}\n",
    "    y_superclass = {\"full\":list()}\n",
    "    y_single_class = {\"URW\":list(),\"CC\":list(),\"Other\":list()}\n",
    "    for v in few:\n",
    "        var = []\n",
    "        try:\n",
    "            y[v[\"superclass\"][0]].append(v[\"y\"])\n",
    "            y_total[v[\"superclass\"][0]].append(v[\"y_total\"])\n",
    "            y_superclass[\"full\"].append(v[\"superclass_embedding\"])\n",
    "            y_single_class[v[\"superclass\"][0]].append(v[\"y_single_class\"])\n",
    "        except:\n",
    "            pass\n",
    "        for j in v[val]:\n",
    "            var.append(j)\n",
    "        if val2 != None:\n",
    "            for j in v[val2]:\n",
    "                var.append(j)\n",
    "        \n",
    "        var = numpy.concatenate(var)\n",
    "        var = var.flatten()\n",
    "        input_x[\"full\"].append(var)\n",
    "        try:\n",
    "            input_x[v[\"superclass\"]].append(var)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    for key,value in input_x.items():\n",
    "        input_x[key] = numpy.array(value)\n",
    "    for key,value in y.items():\n",
    "        y[key] = numpy.array(value)\n",
    "    for key,value in y_total.items():\n",
    "        y_total[key] = numpy.array(value)\n",
    "    for key,value in y_superclass.items():\n",
    "        y_superclass[key] = numpy.array(value)\n",
    "    for key,value in y_single_class.items():\n",
    "        y_single_class[key] = numpy.array(value)\n",
    "    \n",
    "    return(input_x,y,y_total,y_superclass,y_single_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset extraction\n",
    "The feature extraction process is very time-consuming, so the script ensures that extracted embeddings are saved, allowing the extraction to be performed only once.\n",
    "#### Important\n",
    "Embeddings .pkl dataset should match the files in the dataset defined folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "file = Path(\"./train.pkl\")\n",
    "if not file.exists():\n",
    "    with open(trainjson,\"r\",encoding=\"utf8\") as f:\n",
    "        trainreadjson = json.load(f)\n",
    "    train = trainreadjson#[0:trainsize]\n",
    "    preprocess_embeddings(train)\n",
    "    with open('train.pkl', 'wb') as f:\n",
    "        pickle.dump(train,f)\n",
    "file = Path(\"./test.pkl\")\n",
    "if not file.exists():\n",
    "    with open(testjson,\"r\",encoding=\"utf8\") as f:\n",
    "        testreadjson = json.load(f)\n",
    "    test = testreadjson\n",
    "    preprocess_embeddings(test)\n",
    "    with open('test.pkl', 'wb') as f:\n",
    "        pickle.dump(test,f)\n",
    "file = Path(\"./dev.pkl\")\n",
    "if not file.exists():\n",
    "    with open(devjson,\"r\",encoding=\"utf8\") as f:\n",
    "        devreadjson = json.load(f)\n",
    "    dev = devreadjson\n",
    "    preprocess_embeddings(dev)\n",
    "    with open('dev.pkl', 'wb') as f:\n",
    "        pickle.dump(dev,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open('objs25.pkl', 'rb') as f:\n",
    "#        train,test,dev = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('train.pkl', 'wb') as f:\n",
    "#        pickle.dump(train,f)\n",
    "#with open('test.pkl', 'wb') as f:\n",
    "#        pickle.dump(test,f)\n",
    "#with open('dev.pkl', 'wb') as f:\n",
    "#        pickle.dump(dev,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.pkl', \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "with open('test.pkl', \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "with open('dev.pkl', \"rb\") as f:\n",
    "    dev = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainmode = 2\n",
    "\n",
    "# mode 0 -> independent sentencies\n",
    "# mode 2 -> main sentence + 2 preceeding sentences window\n",
    "\n",
    "# this was used for another network to predict argoument and narrative\n",
    "# its unused\n",
    "\n",
    "input_x,y,y_total,y_superclass,y_single_class = convert_to_model_special(train,trainmode)\n",
    "input_dev,y3,y3_total,y3_superclass,y3_single_class = convert_to_model_special(dev,trainmode)\n",
    "input_validation,y2,y_total2,y2_superclass,y2_single_class = convert_to_model_special(test,trainmode)\n",
    "\n",
    "# this code was for normalizzation part, but at the end it was discarded\n",
    "normalizer = lambda x:x\n",
    "\n",
    "# variables to get shapes for input and output for neural networks\n",
    "anothervar = input_x[\"full\"].shape[1]\n",
    "shape_sup = len(y_superclass[\"full\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this early stopped was unreliable\n",
    "# its unused/disabled(watch the condition) \n",
    "\n",
    "class stopper(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        val_loss = logs[\"val_loss\"]\n",
    "        if val_loss < 0.000:\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "# input layer shape\n",
    "inpt = layers.Input(shape=(anothervar,))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argoument classificator, this was used\n",
    "# a top-down approach, but it is unused\n",
    "def build_CL():\n",
    "    layers = keras.layers\n",
    "    NN1 = keras.models.Model\n",
    "    ##########\n",
    "    l8 = layers.Dense(anothervar//30, activation='relu')(inpt)\n",
    "    l8 = layers.Dropout(0.3)(l8)\n",
    "    l8 = layers.Dense(shape_sup, activation='linear')(l8)\n",
    "    simg_predictions = layers.Dense(shape_sup, activation='sigmoid')(l8)\n",
    "#    ##########\n",
    "    NN1 = NN1(inpt,simg_predictions)\n",
    "    return NN1\n",
    "\n",
    "retrain = False\n",
    "if not retrain:\n",
    "    listt_CC = []\n",
    "for j in range(1):\n",
    "    iter = input_x[\"full\"]\n",
    "    y_iter = y_superclass[\"full\"]\n",
    "    if not retrain:\n",
    "        CL = build_CL()\n",
    "        CL.compile(loss=['binary_crossentropy'], optimizer='adam', metrics=['accuracy'])\n",
    "        history = CL.fit(x=iter,y=y_iter,validation_data=(normalizer(input_validation[\"full\"]),y2_superclass[\"full\"]),batch_size=128, epochs=8,callbacks=[])\n",
    "        retrain=True\n",
    "    else:\n",
    "        CL = CL\n",
    "    #cerco di ridurre il più possibile la loss di validation\n",
    "    #CL.compile(loss=['binary_crossentropy'], optimizer='sgd', metrics=['accuracy'])                                        #1000\n",
    "    #history = CL.fit(x=iter,y=y_iter,validation_data=(normalizer(input_validation[\"full\"]),y2_superclass[\"full\"]),batch_size=8500, epochs=1000,callbacks=[])#1*(1+j))\n",
    "    #CL.acc = history.history['val_accuracy']\n",
    "    listt_CC.append(CL)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real neural network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset convertion to numpy for keras\n",
    "input_x,y,y_total = convert_to_model(train,trainmode)\n",
    "input_dev,y3,y3_total = convert_to_model(dev,trainmode)\n",
    "input_validation,y2,y_total2 = convert_to_model(test,trainmode)\n",
    "shapey = len(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network construction\n",
    "\n",
    "def build_NN1(relu_layer=15,dropout=0.4):\n",
    "    layers = keras.layers\n",
    "    NN1 = keras.models.Model\n",
    "    ##########\n",
    "    l8 = layers.Dense(anothervar//relu_layer, activation='relu')(inpt)\n",
    "    l8 = layers.Dropout(dropout)(l8)\n",
    "    l8 = layers.Dense(shapey, activation='linear')(l8)\n",
    "    simg_predictions = layers.Dense(shapey, activation='softmax')(l8)\n",
    "    ##########\n",
    "    NN1 = NN1(inpt,simg_predictions)\n",
    "    return NN1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an ensemble method was planned, but discarded due to no performance benefits\n",
    "# editing div, just divides the dataset in n parts, if set to one, no real division happens\n",
    "div = 1\n",
    "leng_input = len(input_x)//div\n",
    "input_x = [input_x[j*leng_input:(j+1)*leng_input] for j in range(div)]\n",
    "y = [y[j*leng_input:(j+1)*leng_input] for j in range(div)]\n",
    "y_tot = [y_total[j*leng_input:(j+1)*leng_input] for j in range(div)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "You can stop this any time without repercussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network could be retrained\n",
    "# but each time is trained from zero\n",
    "\n",
    "relu_layer = 30\n",
    "dropout = 0.4\n",
    "\n",
    "retrain = False\n",
    "if not retrain:\n",
    "    listt_NN1 = []\n",
    "# ensemble part, but its run only once since div is 1\n",
    "for j in range(div):\n",
    "\n",
    "    # part of dataset extraction for independent enseble modulse\n",
    "    # since div is 1, this is the entire dataset\n",
    "\n",
    "    iter = input_x[j]\n",
    "    y_iter = y[j]\n",
    "    if not retrain:\n",
    "        # neural network building\n",
    "        NN1 = build_NN1(relu_layer=relu_layer,dropout=dropout)\n",
    "        \n",
    "        # adam optimizer training, with cross entropy\n",
    "        NN1.compile(loss=['binary_crossentropy'], optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        # training, for few epochs,\n",
    "        # the normalizer doent do anything,\n",
    "        # the validation is the dataset used for tests\n",
    "        history = NN1.fit(shuffle = True,x=normalizer(iter),y=y_iter,validation_data=(normalizer(input_validation),y2),batch_size=128, epochs=10,callbacks=[stopper()])\n",
    "        retrain=True\n",
    "    else:\n",
    "        NN1 = NN1\n",
    "        \n",
    "    # slow training with sgd\n",
    "    # the early stopping was done manually when the loss couldn't decrease anymore\n",
    "    # you can go less with the epochs or more as long validation loss continues decreasing\n",
    "    NN1.compile(loss=['binary_crossentropy'], optimizer='sgd', metrics=['accuracy'])                                        \n",
    "    history = NN1.fit(shuffle = True,x=normalizer(iter),y=y_iter,validation_data=(normalizer(input_validation),y2),batch_size=8500, epochs=4000,callbacks=[stopper()])#1*(1+j))\n",
    "    NN1.acc = history.history['val_accuracy']\n",
    "    listt_NN1.append(NN1)\n",
    "\n",
    "# various plot if we wnd the epochs to the last one(improbable)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codice non utilizzato per il modello finale, ma su cui sono stati fatti dei test di predizione del numero di labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained Neural network model to re-import if needed, \n",
    "# you need to change the name to load the correct network for your language\n",
    "\n",
    "load_from_files = False\n",
    "if load_from_files:\n",
    "    # model re-import\n",
    "    NN1 = keras.saving.load_model(\"NN1_BG1.keras\")\n",
    "    CL = keras.saving.load_model(\"CL_BG1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was a list of models for old ensemble approach, now it just contain a main model\n",
    "listt_NN1 = [NN1]\n",
    "# this isnt used for predictions,\n",
    "listt_CC = [CL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of probabilities and post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the list of subnarrattives for later use\n",
    "l=list(var)\n",
    "print(l)\n",
    "\n",
    "# this function does the post-processing\n",
    "\n",
    "# validation inputs are the dataset to predict\n",
    "# input_validation is the x of the dataset to predict\n",
    "# validat_x its the dictionary containing the file info\n",
    "# filename is the file used to write predictions\n",
    "# the thresholds are the first one fo URW argoument, the second one for CC\n",
    "# if less than threshold, the subnarrative is excluded\n",
    "def pred_to_json_ensemble(list_NN1,list_CL,input_validation,validat_x,filename\n",
    "                          ,treeshold1,treeshold2):\n",
    "    # test parameters for manipulating probabilitites, aren't really used\n",
    "    # this threshold was to cut probabilities for argoumtnt detection\n",
    "    min_treeshold = 0.0\n",
    "    # this was used to reduce Other class prevalence\n",
    "    reductionthreshold = 0\n",
    "    # the normalizer does nothing, it should have been used for normalizzation of input dataset\n",
    "    inp = normalizer(input_validation)\n",
    "    # ensemble predictions, but this its runned only once, so no ensemble\n",
    "    k = list_NN1[0].predict(inp,verbose=0)\n",
    "    for v in list_NN1[1:]:\n",
    "        k = k + v.predict(inp,verbose=0)\n",
    "    res = k\n",
    "    # this was for argument prediction, its unused\n",
    "    k = list_CL[0].predict(inp,verbose=0)\n",
    "    for v in list_CL[1:]:\n",
    "        k = k + v.predict(inp,verbose=0)\n",
    "    res_CC = k\n",
    "    \n",
    "    # packing everything after predictions\n",
    "    res = (res,res_CC)\n",
    "    #res = (NN1.predict(inp,verbose=0),NN2.predict(inp,verbose=0))\n",
    "    summprob = numpy.zeros(len(l))\n",
    "    summclass =  numpy.zeros(3)\n",
    "    newdict = []\n",
    "    n_samples = 0\n",
    "\n",
    "    # iteration for each sentence of the dataset to predtict\n",
    "    for v,k,file in zip(res[0],res[1],validat_x):\n",
    "        n_samples +=1\n",
    "\n",
    "        # sum of subnarratives probabilities for each sentence of a file\n",
    "        summprob += v\n",
    "        # sum of argoument probablities(unused)\n",
    "        summclass += k\n",
    "        # old multiplicative method\n",
    "        #summprob += numpy.log(v)\n",
    "\n",
    "        # end of a file, start processing of the probabilities of the said file\n",
    "        if file[\"fulltext\"] == \"1\" or file[\"fulltext\"] == 1:\n",
    "            #print(file[\"file\"])\n",
    "\n",
    "            # normalizzation to put everything in 0 to 1\n",
    "            summprob /= n_samples\n",
    "            summclass /= n_samples\n",
    "\n",
    "            # old multiplicative method\n",
    "            #summprob = numpy.exp(summprob)  # Converti in probabilità\n",
    "            #summprob = summprob / numpy.sum(summprob)\n",
    "            #print(summprob)\n",
    "            #print(summclass)\n",
    "            \n",
    "            # map the probabilities to the correspontig subnarrative\n",
    "            probdict = {stringa : 0 for i, stringa in enumerate(l)}\n",
    "            i = 0\n",
    "            while i<len(l):\n",
    "                probdict[l[i]] = summprob[i]\n",
    "                i+=1\n",
    "\n",
    "            # this was use for argument extraction between URW,CC,Other,\n",
    "            # this part goes from subnarratives to argoument, and the greater was chosen\n",
    "            UWR = 0\n",
    "            Other = 0\n",
    "            CC = 0\n",
    "            for key,value in probdict.items():\n",
    "                if key in superclassmap['URW']:\n",
    "                    if value < min_treeshold:\n",
    "                            Other -= reductionthreshold*value\n",
    "                    else:\n",
    "                            UWR += value\n",
    "                elif key in superclassmap['CC']:\n",
    "                    if value < min_treeshold:\n",
    "                            Other -= reductionthreshold* value\n",
    "                    else:\n",
    "                            CC += value\n",
    "                else:\n",
    "                    Other +=  value\n",
    "            \n",
    "            # since argoument is given, i can override them looking for the filename\n",
    "            if \"CC\" in file[\"file\"] :\n",
    "                UWR = 0\n",
    "            else:\n",
    "                try:\n",
    "                    if \"CC\" in file[\"superclass\"]:\n",
    "                       UWR = 0\n",
    "                    #print(file[\"superclass\"])\n",
    "                except:\n",
    "                    pass  \n",
    "\n",
    "            # multiple filenames for URW\n",
    "            if \"URW\" in file[\"file\"] or \"UA\" in file[\"file\"] or \"RU_\" in file[\"file\"]:\n",
    "                CC = 0\n",
    "            else:\n",
    "                try:\n",
    "                    if \"URW\" in file[\"superclass\"] or \"UA\" in file[\"superclass\"] or \"RU_\" in file[\"superclass\"]:\n",
    "                       CC = 0\n",
    "                except:\n",
    "                    pass \n",
    "                 \n",
    "            # chosing the max argoument\n",
    "            a_list = [UWR,CC,Other]\n",
    "\n",
    "            # this was te approach when the argument classifier was used, predictions are just \n",
    "            # discarded since this line is commented\n",
    "            #a_list = [UWR +summclass[0],CC+summclass[1],Other+summclass[2]]\n",
    "            \n",
    "\n",
    "            a_list = [x for x in a_list]\n",
    "            #print(a_list)\n",
    "\n",
    "            # taking out the higest argument\n",
    "            superclass = a_list.index(max([x for x in a_list]))\n",
    "            another_map = [\"URW\",\"CC\",\"Other\"]\n",
    "            dic = {}\n",
    "            labelcontainer = []\n",
    "            tmptreeshold1 = treeshold1\n",
    "            tmptreeshold2 = treeshold2\n",
    "\n",
    "            # cutting out low probabilities\n",
    "            while len(labelcontainer) == 0:\n",
    "                for key,value in probdict.items():\n",
    "                    if key in superclassmap['URW']:\n",
    "                        if superclass == 0 and value >= tmptreeshold1:\n",
    "                            labelcontainer.append(key)\n",
    "                    elif key in superclassmap['CC']:\n",
    "                        if superclass == 1 and value >= tmptreeshold2:\n",
    "                            labelcontainer.append(key)\n",
    "                    else:\n",
    "                        if superclass == 2:\n",
    "                            labelcontainer.append(key)\n",
    "                # fallback if probabilities are too low to have at least one probability\n",
    "                tmptreeshold1 -= 0.001\n",
    "                tmptreeshold2 -= 0.001\n",
    "\n",
    "            # putting the predictions for those files for writing them\n",
    "            dic[\"labels\"] = labelcontainer\n",
    "            dic[\"class\"] = another_map[superclass]\n",
    "            dic[\"file\"] = file[\"file\"]\n",
    "\n",
    "            # put everithing in a list for later processing\n",
    "            newdict.append(dic)\n",
    "            \n",
    "            # reset for new file processing\n",
    "            summprob = numpy.zeros(len(l))\n",
    "            summclass =  numpy.zeros(3)\n",
    "            n_samples = 0\n",
    "    \n",
    "    # writing everithing to predicton file for semeval format\n",
    "    to_write = \"\"\n",
    "    patt = r\"(.*):([^:]*)\"\n",
    "    for elem in newdict:\n",
    "        if \"Other\" in elem[\"class\"]:\n",
    "            to_write += elem[\"file\"] +\"\\tOther\\tOther\\n\"\n",
    "        else:\n",
    "            to_write += elem[\"file\"] +\"\\t\"+\";\".join([re.match(patt,x).group(1) for x in elem[\"labels\"]])+\"\\t\"+\"\".join(elem[\"labels\"])+\"\\n\"\n",
    "\n",
    "    #print(to_write)\n",
    "    with open(filename, \"w\") as outfile: \n",
    "        outfile.write(to_write)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold automatized finder\n",
    "this uses semeval evaluation script\n",
    "Avoid running this is the sample dataset since its useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "tr1 = 0\n",
    "tr2 = 0\n",
    "min = 0\n",
    "sdv2 = 0\n",
    "save = None\n",
    "command = [\n",
    "    'python.exe', \n",
    "    'subtask2_scorer.py', \n",
    "    '-p', 'validation25.txt', \n",
    "    '-g', './dataset/test.txt', \n",
    "    '-f', 'subtask2_subnarratives.txt', \n",
    "    '-c', 'subtask2_narratives.txt'\n",
    "]\n",
    "# increasing threshold \n",
    "while tr1 < 0.4:\n",
    "    tr2 = 0\n",
    "    while tr2 < 0.4:\n",
    "        # prediction\n",
    "        pred_to_json_ensemble(listt_NN1,listt_CC,input_validation,test,\"validation25.txt\",tr1,tr2)\n",
    "        # running evaluation file\n",
    "        result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        # taking score\n",
    "        result = result.stdout.split(\"\\n\")[2].split(\" \")\n",
    "        result1 = float(result[1])\n",
    "        sdv = result[2][1:5]\n",
    "        sdv = float(sdv)\n",
    "        \n",
    "        # getting the relatively better threshold combination\n",
    "        if result1 > min:\n",
    "            # preferring lower std between same scores \n",
    "            if not (result1 - min < 0.01 and sdv2 < sdv):\n",
    "                min = result1\n",
    "                sdv2 = sdv\n",
    "                save = (tr1,tr2)\n",
    "                print(result1)\n",
    "        tr2 += 0.01\n",
    "    tr1 += 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tr1,tr2 = save\n",
    "# for manual threshold just de-comment\n",
    "#tr1 = 0.2\n",
    "#tr2 = 0.15\n",
    "\n",
    "pred_to_json_ensemble(listt_NN1,listt_CC,input_x,train,\"training25.txt\",tr1,tr2)\n",
    "pred_to_json_ensemble(listt_NN1,listt_CC,input_validation,test,\"validation25.txt\",tr1,tr2)\n",
    "pred_to_json_ensemble(listt_NN1,listt_CC,input_dev,dev,\"predictions25.txt\",tr1,tr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Manual save when models are optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN1.save(\"NN1_PT2.keras\")\n",
    "#CL.save(\"CL_PT2.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually registered configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "RU\n",
    "//5\n",
    "softmax\n",
    "treeshold1 = 0.1\n",
    "trainmode = 0\n",
    "\n",
    "Evaluation Results:\n",
    "F1@coarse: 0.713 (0.276)\n",
    "F1@fine: 0.535 (0.328)\n",
    "\n",
    "PT\n",
    "\n",
    "//15\n",
    "sigmoid\n",
    "trainmode = 0\n",
    "min_treeshold = 0.075\n",
    "treeshold1 = 0.15\n",
    "treeshold2 = 0.2\n",
    "\n",
    "Evaluation Results:\n",
    "F1@coarse: 0.723 (0.277)\n",
    "F1@fine: 0.512 (0.281)\n",
    "\n",
    "\n",
    "BG\n",
    "//10\n",
    "sigmoid\n",
    "treeshold1 = 0.14\n",
    "treeshold2 = 0.1\n",
    "\n",
    "F1@coarse: 0.683 (0.294)\n",
    "F1@fine: 0.533 (0.322)\n",
    "\n",
    "HI\n",
    "//15\n",
    "dropout 0.4\n",
    "softmax\n",
    "0.04\n",
    "0.06\n",
    "Evaluation Results:\n",
    "F1@coarse: 0.601 (0.301)\n",
    "F1@fine: 0.451 (0.329)\n",
    "\n",
    "//EN\n",
    "//30\n",
    "dropout 0.4\n",
    "softmax\n",
    "0.515\n",
    "F1@coarse: 0.650 (0.332)\n",
    "F1@fine: 0.520 (0.353)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
